{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RL2nd.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTNU1mwGB1ZD"
      },
      "source": [
        "**Initialise**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TZefME0MTvA"
      },
      "source": [
        "\n",
        "# the code is based on https://github.com/higgsfield/RL-Adventure/blob/master/4.prioritized%20dqn.ipynb\n",
        "\n",
        "# imports\n",
        "import gym\n",
        "import collections\n",
        "import math, random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.autograd as autograd \n",
        "\n",
        "from collections import deque\n",
        "# hyperparameters\n",
        "learning_rate = 0.0003\n",
        "gamma         = 0.99\n",
        "buffer_limit  = 50000\n",
        "batch_size    = 32\n",
        "video_every   = 25\n",
        "print_every   = 5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G4PqSaKwOcO6"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available()\r\n",
        "Variable = lambda *args, **kwargs: autograd.Variable(*args, **kwargs).cuda() if USE_CUDA else autograd.Variable(*args, **kwargs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8tS8Cn9UOcQ8"
      },
      "source": [
        "\r\n",
        "class PrioritizedBuffer(object):\r\n",
        "    def __init__(self, capacity, prob_alpha=0.6):\r\n",
        "        self.prob_alpha = prob_alpha\r\n",
        "        self.capacity   = capacity\r\n",
        "        self.buffer     = []\r\n",
        "        self.pos        = 0\r\n",
        "        self.priorities = np.zeros((capacity,), dtype=np.float32)\r\n",
        "    \r\n",
        "    def push(self, state, action, reward, next_state, done):\r\n",
        "        assert state.ndim == next_state.ndim\r\n",
        "        state      = np.expand_dims(state, 0)\r\n",
        "        next_state = np.expand_dims(next_state, 0)\r\n",
        "        \r\n",
        "        max_prio = self.priorities.max() if self.buffer else 1.0\r\n",
        "        \r\n",
        "        if len(self.buffer) < self.capacity:\r\n",
        "            self.buffer.append((state, action, reward, next_state, done))\r\n",
        "        else:\r\n",
        "            self.buffer[self.pos] = (state, action, reward, next_state, done)\r\n",
        "        \r\n",
        "        self.priorities[self.pos] = max_prio\r\n",
        "        self.pos = (self.pos + 1) % self.capacity\r\n",
        "    \r\n",
        "    def sample(self, batch_size, beta=0.4):\r\n",
        "        if len(self.buffer) == self.capacity:\r\n",
        "            prios = self.priorities\r\n",
        "        else:\r\n",
        "            prios = self.priorities[:self.pos]\r\n",
        "        \r\n",
        "        probs  = prios ** self.prob_alpha\r\n",
        "        probs /= probs.sum()\r\n",
        "        \r\n",
        "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\r\n",
        "        samples = [self.buffer[idx] for idx in indices]\r\n",
        "        \r\n",
        "        total    = len(self.buffer)\r\n",
        "        weights  = (total * probs[indices]) ** (-beta)\r\n",
        "        weights /= weights.max()\r\n",
        "        weights  = np.array(weights, dtype=np.float32)\r\n",
        "        \r\n",
        "        batch       = list(zip(*samples))\r\n",
        "        states      = np.concatenate(batch[0])\r\n",
        "        actions     = batch[1]\r\n",
        "        rewards     = batch[2]\r\n",
        "        next_states = np.concatenate(batch[3])\r\n",
        "        dones       = batch[4]\r\n",
        "        \r\n",
        "        return states, actions, rewards, next_states, dones, indices, weights\r\n",
        "    \r\n",
        "    def update_priorities(self, batch_indices, batch_priorities):\r\n",
        "        for idx, prio in zip(batch_indices, batch_priorities):\r\n",
        "            self.priorities[idx] = prio\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.buffer)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xtFBh1XxOcYW"
      },
      "source": [
        "\r\n",
        "class DQN(nn.Module):\r\n",
        "    def __init__(self, num_inputs, num_actions):\r\n",
        "        super(DQN, self).__init__()\r\n",
        "        \r\n",
        "        self.layers = nn.Sequential(\r\n",
        "            nn.Linear(env.observation_space.shape[0], 128),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(128, 128),\r\n",
        "            nn.ReLU(),\r\n",
        "            nn.Linear(128, env.action_space.n)\r\n",
        "        )\r\n",
        "        \r\n",
        "    def forward(self, x):\r\n",
        "        return self.layers(x)\r\n",
        "    \r\n",
        "    def act(self, state, epsilon):\r\n",
        "        if random.random() > epsilon:\r\n",
        "          with torch.no_grad():\r\n",
        "\r\n",
        "            state   = Variable(torch.FloatTensor(state).unsqueeze(0))\r\n",
        "          q_value = self.forward(state)\r\n",
        "          action  = q_value.max(1)[1].data[0]\r\n",
        "        else:\r\n",
        "            action = random.randrange(env.action_space.n)\r\n",
        "        return action"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XFhBa9eZ7or-"
      },
      "source": [
        "\r\n",
        "env_id = \"Gravitar-ram-v0\"\r\n",
        "env = gym.make(env_id)\r\n",
        "\r\n",
        "current_model = DQN(env.observation_space.shape[0], env.action_space.n)\r\n",
        "target_model  = DQN(env.observation_space.shape[0], env.action_space.n)\r\n",
        "\r\n",
        "if USE_CUDA:\r\n",
        "    current_model = current_model.cuda()\r\n",
        "    target_model  = target_model.cuda()\r\n",
        "    \r\n",
        "optimizer = optim.Adam(current_model.parameters(),learning_rate)\r\n",
        "\r\n",
        "replay_buffer =  PrioritizedBuffer(100000)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU-wzg0D7smZ"
      },
      "source": [
        "def update_target(current_model, target_model):\r\n",
        "    target_model.load_state_dict(current_model.state_dict())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B553uncWOh_t"
      },
      "source": [
        "beta_start = 0.4\r\n",
        "beta_frames = 1000 \r\n",
        "beta_by_frame = lambda frame_idx: min(1.0, beta_start + frame_idx * (1.0 - beta_start) / beta_frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aemAyGTx7vuj"
      },
      "source": [
        "update_target(current_model, target_model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ilw8y0Tt7ybA"
      },
      "source": [
        "def compute_td_loss(batch_size, beta):\r\n",
        "    state, action, reward, next_state, done, indices, weights = replay_buffer.sample(batch_size, beta) \r\n",
        "\r\n",
        "    state      = Variable(torch.FloatTensor(np.float32(state)))\r\n",
        "    next_state = Variable(torch.FloatTensor(np.float32(next_state)))\r\n",
        "    action     = Variable(torch.LongTensor(action))\r\n",
        "    reward     = Variable(torch.FloatTensor(reward))\r\n",
        "    done       = Variable(torch.FloatTensor(done))\r\n",
        "    weights    = Variable(torch.FloatTensor(weights))\r\n",
        "\r\n",
        "    q_values      = current_model(state)\r\n",
        "    next_q_values = target_model(next_state)\r\n",
        "\r\n",
        "    q_value          = q_values.gather(1, action.unsqueeze(1)).squeeze(1)\r\n",
        "    next_q_value     = next_q_values.max(1)[0]\r\n",
        "    expected_q_value = reward + gamma * next_q_value * (1 - done)\r\n",
        "    \r\n",
        "    loss  = (q_value - expected_q_value.detach()).pow(2) * weights\r\n",
        "    prios = loss + 1e-5\r\n",
        "    loss  = loss.mean()\r\n",
        "        \r\n",
        "    optimizer.zero_grad()\r\n",
        "    loss.backward()\r\n",
        "    replay_buffer.update_priorities(indices, prios.data.cpu().numpy())\r\n",
        "    optimizer.step()\r\n",
        "    \r\n",
        "    return loss"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r3B0_GDnP85I",
        "outputId": "c15a6f2d-d8bc-44f6-f58b-43bafb677a4f"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ck-chjFdScJ"
      },
      "source": [
        "**Train**\n",
        "\n",
        "← You can download the videos from the videos folder in the files on the left"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5stHkFq4UztI"
      },
      "source": [
        "# setup the Gravitar ram environment, and record a video every 50 episodes. You can use the non-ram version here if you prefer\n",
        "env = gym.wrappers.Monitor(env, \"drive/My Drive/training/RL2\", video_callable=lambda episode_id: (episode_id%video_every)==0,force=True)\n",
        "\n",
        "# reproducible environment and action spaces, do not change lines 6-11 here (tools > settings > editor > show line numbers)\n",
        "seed = 742\n",
        "torch.manual_seed(seed)\n",
        "env.seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "env.action_space.seed(seed)\n",
        "\n",
        "score    = 0.0\n",
        "marking  = []\n",
        "losses = []\n",
        "all_rewards = []\n",
        "frame_idx=0\n",
        "episode_reward = 0\n",
        "for n_episode in range(int(1e32)):\n",
        "    epsilon = max(0.05, 0.10 - 0.01*(n_episode/200)) # linear annealing from 8% to 1%\n",
        "    s = env.reset()\n",
        "    done = False\n",
        "    score = 0.0\n",
        "    while True:\n",
        "      frame_idx+=1\n",
        "      action = current_model.act(s, epsilon)\n",
        "      \n",
        "\n",
        "      next_state, reward, done, _ = env.step(action)\n",
        "      replay_buffer.push(s, action, reward, next_state, done)\n",
        "      s = next_state\n",
        "\n",
        "      score += reward\n",
        "      if done:\n",
        "        all_rewards.append(score)\n",
        "        episode_reward = 0\n",
        "        break\n",
        "        \n",
        "      if len(replay_buffer) > batch_size:\n",
        "          beta = beta_by_frame(frame_idx)\n",
        "          loss = compute_td_loss(batch_size, beta)\n",
        "          losses.append(loss.item())\n",
        "          \n",
        "      if frame_idx % 1000 == 0:\n",
        "          update_target(current_model, target_model)\n",
        "    # do not change lines 44-48 here, they are for marking the submission log\n",
        "    marking.append(score)\n",
        "    if n_episode%100 == 0:\n",
        "        print(\"marking, episode: {}, score: {:.1f}, mean_score: {:.2f}, std_score: {:.2f}\".format(\n",
        "            n_episode, score, np.array(marking).mean(), np.array(marking).std()))\n",
        "        marking = []\n",
        "\n",
        "    # you can change this part, and print any data you like (so long as it doesn't start with \"marking\")\n",
        "    if n_episode%print_every==0 and n_episode!=0:\n",
        "        target_model.load_state_dict(current_model.state_dict())\n",
        "        print(\"episode: {}, score: {:.1f}, epsilon: {:.2f}\".format(n_episode, score, epsilon))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}